---
title: "Weight Lifting Exercise Data Analysis"
author: "Dongyuan Wu"
date: "`r Sys.Date()`"
output: 
  html_document:
    keep_md: true
---

## Summary

One thing that people regularly do is quantify how *much* of a particular activity they do, but they rarely quantify how *well* they do it. The data in this project is from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The goal of this project is to predict the manner in which they did the exercise. 

The following issues will be described: How to build the model, how to use cross validation, what the expected out of sample error is, and why the choices were made. 20 different test cases will also be predicted by using the prediction model in this project.

## Preperation

```{r, message=FALSE}

library(caret)
library(ggplot2)
library(randomForest)

# change system language
Sys.setlocale("LC_TIME", "English")

# read data
training.raw <- read.csv("pml-training.csv", na.strings=c("NA", "#DIV/0!", ""))
testing<- read.csv("pml-testing.csv", na.strings=c("NA", "#DIV/0!", ""))

```

## Data Processing

```{r}

dim(training.raw)

```

As we can see, in the raw training dataset, there are `r nrow(training.raw)` observations and `r ncol(training.raw)` variables. However, lots of variables are missing values or meaningless in this project. We need to remove them.

```{r}

# remove variables which NAs account for more than 80% values
na.sum <- apply(training.raw, 2, function(x) sum(is.na(x)))
training <- training.raw[, which(na.sum/nrow(training.raw) < 0.8)]

# remove variables which are meaningless in project
training <- training[, -c(1:7)]

dim(training)

```

Now the new training dataset has only `r ncol(training)` variables, including 1 outcome and `r ncol(training)-1` predictors.

## Cross Validation

The original training dataset needs to be split into a sub-training set (80%) and a sub-testing set (20%).

```{r}

set.seed(1111)
intrain <- createDataPartition(y=training$classe, p=0.8, list=FALSE)
subtrain <- training[intrain, ] 
subtest <- training[-intrain, ]

dim(subtrain)
dim(subtest)

```

The sub-training set has `r nrow(subtrain)` observations and `r ncol(subtrain)` variables. The sub-testing set has `r nrow(subtest)` observations and `r ncol(subtest)` variables.

## Model Training

We choose to build decision trees due to its better performance in nonlinear settings and easily interpreted.

```{r}

set.seed(9102)
# fit models
modfit <- randomForest(classe ~ ., data=subtrain, type="class")
    
```

We can view the confusion matrix and class errors for the model in sub-training set:

```{r}

modfit$confusion

```

The results seem very good!

## Variable Importance

```{r}

varImpPlot(modfit, sort=TRUE, n.var=min(20, nrow(modfit$importance)),
           main="Varaible Top 20 Importance", pch=16, color="orange")

```

## Prediction

Use the random forest model to predict the sub-testing data, and then view the confusion matrix.

```{r}

prediction.sub <- predict(modfit, subtest)
(confu <- confusionMatrix(prediction.sub, subtest$classe))

```

The accuracy is `r round(confu$overall[1], 4)`, which is good enough. Moreover, the expected out of sample errors by class are very small.

## Prediction for the 20 different test cases

We have 20 different test cases need to be predicted.

```{r}

(prediction <- predict(modfit, testing))

```